<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Pengyuan Wang</title>

    <meta name="author" content="Pengyuan Wang">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Pengyuan Wang
                </p>
                <p>I am a PhD student at <a href="https://www.cs.cit.tum.de/camp/research/computer-vision/">TUM CAMP computer vision group</a> supervised by <a href="https://www.professoren.tum.de/en/navab-nassir"> Prof. Dr. Nassir Navab</a>. My research focuses on object pose estimations and robotic manipulations.
                </p>
                <p style="text-align:center">
                  <a href="pengyuan.wang@tum.de">Email</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=JrPjwOsAAAAJ">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/pengyuanwang93/">Linkedin</a> &nbsp;/&nbsp;
                  <a href="https://github.com/Seasandwpy">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/pengyuan.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/pengyuan.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                 My research interest is in the pose estimation of category-level objects and novel objects, which is important for robotic applications. Also
                  I am interested in vision-language-action models for generic robotic manipulations.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

    <!-- gspose -->
    <tr onmouseout="nuvo_stop()" onmouseover="nuvo_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <img src='images/gspose.svg' width=120% >
        </div>
        <script type="text/javascript">
          function nuvo_start() {
            document.getElementById('nuvo_image').style.opacity = "1";
          }
          function nuvo_stop() {
            document.getElementById('nuvo_image').style.opacity = "0";
          }
          nuvo_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="">
          <span class="papertitle">GS-Pose: Category-Level Object Pose Estimation via Geometric and Semantic Correspondence</span>
        </a>
        <br>
        <strong>Pengyuan Wang</strong>,
        <a href="https://scholar.google.co.jp/citations?user=HS4dPGQAAAAJ">Takuya Ikeda</a>,
        <a href="https://scholar.google.com.au/citations?user=1Vqlm0kAAAAJ">Robert Lee</a>,
        <a href="https://scholar.google.com/citations?user=oC2CnhUAAAAJ">Koichi Nishiwaki</a>
        <br>
        <em>ECCV</em>, 2024
        <br>
        <a href="https://woven-planet.github.io/GS-Pose/">project page </a>
        /
        <a href="https://github.com/woven-planet/GS-Pose">code</a>
        /
        <a href="https://arxiv.org/abs/2311.13777">arXiv</a>
        <p></p>
        <p>
            GS-Pose enables training category-level object pose estimations from <strong> only a few synthetic CAD models </strong>, and achieves comparable
          results with baselines trained on real pose dataset.
        </p>
      </td>
    </tr>

    <!-- housecat6d -->
    <tr onmouseout="nuvo_stop()" onmouseover="nuvo_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <img src='images/housecat6d.png' width=120% >
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="">
          <span class="papertitle">HouseCat6D - A Large-Scale Multi-Modal Category Level 6D Object Perception
Dataset with Household Objects in Realistic Scenarios</span>
        </a>
        <br>
          <a>HyunJun Jung</a>,
          <a>Shun-Cheng Wu</a>,
          <a>Patrick Ruhkamp</a>,
          <a>Guangyao Zhai</a>,
          <a>Hannah Schieber</a>,
          <a>Giulia Rizzoli</a>,
        <strong>Pengyuan Wang</strong>,
        <a>Hongcheng Zhao</a>,
          <a>Lorenzo Garattoni</a>,
        <a>Sven Meier</a>,
          <a>Daniel Roth</a>,
            <a>Nassir Navab</a>,
          <a>Benjamin Busam</a>,
        <br>
          <em>  CVPR  </em>, 2024 &nbsp <font color="red"><strong>(Highlight)</strong></font>
        <br>
        <a href="https://sites.google.com/view/housecat6d">project page</a>
        /
          <a href="https://sites.google.com/view/housecat6d/dataset?authuser=0">dataset</a>
          /
        <a href="https://arxiv.org/abs/2212.10428">arXiv</a>
        <p></p>
        <p>
        A large-scale category-level pose dataset with accurate ground truth pose annotations. The dataset features
            photometrically-challenging categories including transparent and metallic objects, and more diverse object poses in
            the real scenes. Feel free to test your pose estimation methods on our benchmark.
        </p>
      </td>
    </tr>


    <!-- transparent pose -->
    <tr onmouseout="nuvo_stop()" onmouseover="nuvo_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <img src='images/glass9d.png' width=120% >
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="">
          <span class="papertitle">Improving Self-Supervised Learning of Transparent Category Poses
          with Language Guidance and Implicit Physical Constraints</span>
        </a>
        <br>
        <strong>Pengyuan Wang</strong>,
        <a href="https://iridia.ulb.ac.be/~lgarattoni/index.html">Lorenzo Garattoni</a>,
        <a href="">Sven Meier</a>,
        <a href="https://www.professoren.tum.de/en/navab-nassir">Nassir Navab</a>
        <a href="https://www.cs.cit.tum.de/camp/members/benjamin-busam/">Benjamin Busam</a>
        <br>
        <em>RA-L</em>, 2024
        <br>
        <a href="">arXiv</a>
        <p></p>
        <p>
          We propose novel <strong>self-supervision losses</strong> to self-supervise the
          category-level pose estimations for <strong>transparent objects</strong>. Only RGB-D with polarization
        images and no real pose annotations are needed in the  self-supervision stage.
        </p>
      </td>
    </tr>

    <!-- ccd3dr -->
    <tr onmouseout="nuvo_stop()" onmouseover="nuvo_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <img src='images/ccd3dr.png' width=120% >
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="">
          <span class="papertitle">CCD-3DR: Consistent Conditioning in Diffusion for Single-Image 3D
            Reconstruction</span>
        </a>
        <br>
          <a> Yan Di </a>,
          <a>Chenyangguang Zhang</a>,
        <strong>Pengyuan Wang</strong>,
          <a> Guangyao Zhai</a>,
          <a> Ruida Zhang </a>,
          <a> Fabian Manhardt </a>,
          <a href="https://www.cs.cit.tum.de/camp/members/benjamin-busam/">Benjamin Busam</a>,
          <a>Xiangyang Ji</a>,
        <a > Federico Tombari </a>
        <br>
        <em>arXiv</em>, 2023
        <br>
        <a href="https://arxiv.org/abs/2308.07837">arXiv</a>
        <p></p>
        <p>
        CCD-3DR works on generative networks imagining object point clouds from a single RGB
        image. Accurate point clouds are generated by diffusion networks with consistent object center conditioning.
        </p>
      </td>
    </tr>

    <!-- crocps -->
    <tr onmouseout="nuvo_stop()" onmouseover="nuvo_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <img src='images/crocps.png' width=120% >
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="">
          <span class="papertitle">CroCPS: Addressing Photometric Challenges in Self-Supervised
        Category-Level 6D Object Poses with Cross-Modal Learning</span>
        </a>
        <br>
          <strong>Pengyuan Wang</strong>,
        <a href="https://iridia.ulb.ac.be/~lgarattoni/index.html">Lorenzo Garattoni</a>,
        <a href="">Sven Meier</a>,
        <a href="https://www.professoren.tum.de/en/navab-nassir">Nassir Navab</a>,
        <a href="https://www.cs.cit.tum.de/camp/members/benjamin-busam/">Benjamin Busam</a>
        <br>
        <em>BMVC</em>, 2022
        <br>
        <a href="https://arxiv.org/abs/2308.07837">paper</a>
        <p></p>
        <p>
        How to estimate the object pose of metallic objects from RGB-D inputs in presence of depth artifacts?
          CroCPS proposes a multi-modal approach to improve pose estimation accuracy in real scenes for metallic category.
        </p>
      </td>
    </tr>


    <!-- ppp -->
    <tr onmouseout="nuvo_stop()" onmouseover="nuvo_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <img src='images/ppp.jpg' width=100% >
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="">
          <span class="papertitle">Polarimetric Pose Prediction</span>
        </a>
        <br>
        <a href="https://daoyig.github.io/">Daoyi Gao</a>,
        <a> Yitong Li </a>,
        <a>Patrick Ruhkamp</a>,
        <a>Iuliia Skobleva</a>,
        <a>Magdalena Wysock</a>,
        <a> HyunJun Jung</a>,
          <strong>Pengyuan Wang</strong>,
        <a>Arturo Guridi</a>,
        <a href="https://www.cs.cit.tum.de/camp/members/benjamin-busam/">Benjamin Busam</a>
        <br>
        <em>ECCV</em>, 2022
        <br>
        <a href="https://daoyig.github.io/PPPNet/"> project page </a>
        /
        <a href="https://arxiv.org/abs/2112.03810">paper</a>
        /
        <a href="https://github.com/DaoyiG/polarimetric-pose-prediction"> code </a>
        <p></p>
        <p>
        PPP-Net proposes a hybrid model that utilizes polarizaiotn information with physical priors in a data-driven learning
          strategy to improve the accuracy of pose predictions for photometric challenging objects.
        </p>
      </td>
    </tr>



      <!-- phocal -->
    <tr onmouseout="nuvo_stop()" onmouseover="nuvo_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <img src='images/phocal.png' width=120% >
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="">
          <span class="papertitle">PhoCaL: A Multi-Modal Dataset for Category-Level Object Pose
            Estimation with Photometrically Challenging Objects</span>
        </a>
        <br>
          <strong>Pengyuan Wang</strong>*,
        <a> HyunJun Jung</a>*,
        <a>Yitong Li</a>,
        <a>Siyuan Shen</a>,
        <a>Rahul Parthasarathy Srikanth</a>,
         <a href="https://iridia.ulb.ac.be/~lgarattoni/index.html">Lorenzo Garattoni</a>,
        <a href="">Sven Meier</a>,
        <a href="https://www.professoren.tum.de/en/navab-nassir">Nassir Navab</a>,
        <a href="https://www.cs.cit.tum.de/camp/members/benjamin-busam/">Benjamin Busam</a>
        <br>
        <em>CVPR</em>, 2022
        <br>
        <a href="https://www.campar.in.tum.de/public_datasets/2022_cvpr_wang/"> project page </a>
        /
        <a href="https://www.campar.in.tum.de/public_datasets/2022_cvpr_wang/"> dataset </a>
        /
        <a href="https://arxiv.org/abs/2205.08811"> arXiv </a>
        <p></p>
        <p>
      We propose a novel method to annotate object poses accurately with
          a robot arm, especially we design a calibration pipeline for hand-eye calibration of the cameras.
          Our method annotates the object poses accurately, even for photometrically-challenging
          objects such as glasses and cutlery.
        </p>
      </td>
    </tr>

      <!-- demograsp -->
    <tr onmouseout="nuvo_stop()" onmouseover="nuvo_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <img src='images/demograsp.jpg' width=120% >
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="">
          <span class="papertitle">DemoGrasp: Few-Shot Learning for Robotic Grasping
      with Human Demonstration</span>
        </a>
        <br>
          <strong>Pengyuan Wang</strong>*,
        <a>Fabian Manhardt</a>*,
        <a>Luca Minciullo</a>,
         <a href="https://iridia.ulb.ac.be/~lgarattoni/index.html">Lorenzo Garattoni</a>,
        <a href="">Sven Meier</a>,
        <a href="https://www.professoren.tum.de/en/navab-nassir">Nassir Navab</a>,
        <a href="https://www.cs.cit.tum.de/camp/members/benjamin-busam/">Benjamin Busam</a>
        <br>
        <em>IROS</em>, 2021
        <br>
        <a href="https://arxiv.org/abs/2112.02849"> arXiv </a>
        <p></p>
        <p>
      DemoGrasp proposes to learn robotic grasping from human demonstrations.
          In the method we learn the grasping pose and object shape jointly in a
          demonstration sequence. Afterwards we propose a novel object pose estimator
          to get object poses in the real scenes as well as the grasping point.

        </p>
      </td>
    </tr>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <h2>Side Projects</h2>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="20"><tbody>

          <!-- demograsp -->
    <tr onmouseout="nuvo_stop()" onmouseover="nuvo_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <img src='images/visualprompting.png' width=120% >
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <span class="papertitle">Unofficial Implementation of <a href="https://openaccess.thecvf.com/content/ICCV2023/papers
        /Shtedritski_What_does_CLIP_know_about_a_red_circle_Visual_prompt_ICCV_2023_paper.pdf">  CLIP Visual Prompting </a></span>
              <br>
        <a href="https://github.com/Seasandwpy/CLIP_VisualPrompting"> Github Code </a>
      <br>
        <p>
     Visual prompting recognizes the keypoint positions in the images by specifying the keypoint names with CLIP. The method is achieved
          by drawing circles around the keypoints. We unofficially implement the method with CLIP models and optimal transport functions, and provide
          a testing example in github.
        </p>

      </td>
    </tr>




          </tbody></table>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Feel free to steal this website's <a href="https://github.com/jonbarron/jonbarron_website">source code</a>. <strong>Do not</strong> scrape the HTML from this page itself, as it includes analytics tags that you do not want on your own website &mdash; use the github code instead. Also, consider using <a href="https://leonidk.com/">Leonid Keselman</a>'s <a href="https://github.com/leonidk/new_website">Jekyll fork</a> of this page.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
